<!DOCTYPE html>
<html lang="en">

<head>
  <title>David Saffo - Personal Portfolio</title>
  <meta charset="UTF-8">
  <meta name="description" content="David Saffo - Personal Portfolio">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- Favicon -->
  <link href="img/favicon.png" rel="shortcut icon" />

  <!-- Preconnect to external domains -->
  <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="preconnect" href="https://stackpath.bootstrapcdn.com" crossorigin>

  <!-- Google Fonts (non-blocking) -->
  <link href="https://fonts.googleapis.com/css?family=Ubuntu&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
  <link rel="stylesheet" href="css/font-awesome.min.css" />
  <link rel="stylesheet" href="css/flaticon.css" />
  <link rel="stylesheet" href="css/academicons.min.css" />

  <!-- Boot Strap -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <link rel="stylesheet" href="css/style.css" />

<script defer src="https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-154499817-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-154499817-1');
  </script>


</head>

<body>

  <div class="container">
    <div class="row">
      <div class="col-lg-3">
        <div class="header sticky-top">
          <svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 500 500" width="100%" id="blobSvg" style="opacity: 1;" transform="rotate(0)" filter="blur(0px)"><image x="0" y="0" width="100%" height="100%" clip-path="url(#shape)" href="https://dsaffo.dev/img/20220403_155045-modified.png" preserveAspectRatio="none"></image>                        <defs>                        <linearGradient id="gradient" x1="0%" y1="0%" x2="0%" y2="100%">                            <stop offset="0%" style="stop-color: rgb(238, 205, 163);"></stop>                            <stop offset="100%" style="stop-color: rgb(239, 98, 159);"></stop>                        </linearGradient>                        </defs>                                            <clipPath id="shape"><path id="blob" fill="url(#gradient)">                            <animate attributeName="d" dur="20000ms" repeatCount="indefinite" values="M440.5,320.5Q418,391,355.5,442.5Q293,494,226,450.5Q159,407,99,367Q39,327,31.5,247.5Q24,168,89,125.5Q154,83,219.5,68Q285,53,335.5,94.5Q386,136,424.5,193Q463,250,440.5,320.5Z;M453.78747,319.98894Q416.97789,389.97789,353.96683,436.87838Q290.95577,483.77887,223.95577,447.43366Q156.95577,411.08845,105.64373,365.97789Q54.33169,320.86732,62.67444,252.61056Q71.01719,184.3538,113.01965,135.21007Q155.02211,86.06634,220.52211,66.46683Q286.02211,46.86732,335.5,91.94472Q384.97789,137.02211,437.78747,193.51106Q490.59704,250,453.78747,319.98894Z;M411.39826,313.90633Q402.59677,377.81265,342.92059,407.63957Q283.24442,437.46649,215.13648,432.5428Q147.02853,427.61911,82.23325,380.9572Q17.43796,334.29529,20.45223,250.83809Q23.46649,167.38089,82.5856,115.05707Q141.70471,62.73325,212.19045,63.73015Q282.67618,64.72705,352.67308,84.79839Q422.66998,104.86972,421.43486,177.43486Q420.19974,250,411.39826,313.90633Z;M440.5,320.5Q418,391,355.5,442.5Q293,494,226,450.5Q159,407,99,367Q39,327,31.5,247.5Q24,168,89,125.5Q154,83,219.5,68Q285,53,335.5,94.5Q386,136,424.5,193Q463,250,440.5,320.5Z;"></animate>                        </path></clipPath></svg>
          <div class="social-links">
            <a href="https://www.linkedin.com/in/david-saffo-a2408810a/" target="_blank"><i class="fa fa-linkedin"></i></a>
            <a href="https://scholar.google.com/citations?user=P6Uk92EAAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/dsaffo" target="_blank"><i class="fa fa-github"></i></a>
          </div>
          <div class="title">
            <h5>David Saffo Ph.D.</h5>
            <h6>davidysaffo@gmail.com</h6>
          </div>
          <div class="anchors">
            <div class="anchor"><a href="#about">About</a></div>
            <div class="anchor"><a href="#projects">Projects</a></div>
            <div class="anchor"><a href="#publications">Publications</a></div>
          </div>
          <div style="margin-top: 10px">
            <a href="David_Saffo_CV.pdf" target="_blank" class="site-btn">Download CV</a>
          </div>
        </div>
      </div>
      <div class="col-lg-9">
        <div id="about" class="about">
          <div class="row">
            <div class="col-lg-6">
              <h2>Hey I'm David,</h2>
            </div>

          </div>
          <h6>I am a researcher and software engineer interested in novel applications of data visualizations, agentic-systems, immersive analytics, AR/VR applications, and the tools we use to build them.</h6>
          <h4>My Keywords: </h4>
          <p>Information Visualization, VR/AR Visualization, Immersive Analytics, Collaboration, Cross-Platform Interaction, Quantitative Methods Human Computer Interaction, User Centered Design, Detroit, TTRPGs, 3D Printing, Board Games, Red Pandas</p>
        </div>
        <div id="projects" class="projects">
          <h2>Projects</h2>
          <div class="line"></div>
         


          <div class="project">
            <div class="row">
              <div class="col-lg-5 teaser">
                <a href="https://jpmorganchase.github.io/anu/" target="_blank">
                  <img src="img/anu.gif" alt="Anu" loading="lazy">
                </a>
              </div>
              <div class="col-lg-7">
                <div class="row">
                  <div class="col-lg-5" style="margin-right: -50px">
                    <a href="https://jpmorganchase.github.io/anu/" target="_blank">
                      <h5>Anu</h5>
                    </a>
                  </div>
                  <div class="col-lg-3">
                    <a class="github-button" href="https://github.com/jpmorganchase/anu" aria-label="Visit the repo"></a>
                    <a class="github-button" href="https://github.com/jpmorganchase/anu" data-icon="octicon-star" data-show-count="true" aria-label="Star Anu on GitHub"></a>
                  </div>
                </div>
                <p>Anu is an immersive data visualization toolkit built on Babylon.js and WebXR. Inspired by D3's data-binding patterns, Anu enables developers to create 3D and VR data visualizations through dynamic scene graph manipulation, pre-fabricated elements, and spatial interactions for web-based immersive analytics.</p>
              </div>
            </div>
          </div>

          <div class="project">
            <div class="row">
              <div class="col-lg-5 teaser">
                <a href="https://iadesign.space/" target="_blank">
                  <img src="img/iadesign.PNG" alt="IADesign.Space" loading="lazy">
                </a>
              </div>
              <div class="col-lg-7">
                <div class="row">
                  <div class="col-lg-5" style="margin-right: -50px">
                    <a href="https://iadesign.space/" target="_blank">
                      <h5>IADesign.Space</h5>
                    </a>
                  </div>
                  <div class="col-lg-3">
                    <a class="github-button" href="https://github.com/VisDunneRight/IADesign.Space" aria-label="Visit the repo"></a>
                    <a class="github-button" href="https://github.com/VisDunneRight/IADesign.Space" data-icon="octicon-star" data-show-count="true" aria-label="Star IADesign.Space on GitHub"></a>
                  </div>
                </div>
                <p>IADesign.Space is an interactive browser for exploring the immersive analytics design space. This comprehensive resource catalogs and organizes research papers across multiple dimensions including spatial presentation, visual presentation, immersive technologies, and contribution types to support researchers and practitioners in understanding the evolving landscape of immersive data visualization.</p>
              </div>
            </div>
          </div>

           <div class="project">
            <div class="row">
              <div class="col-lg-5 teaser">
                <a href="https://github.com/dsaffo/VRxD" target="_blank">
                  <img src="img/L3_Interactions.gif" alt="VRxD Interaction Sharing" loading="lazy">
                </a>
              </div>
              <div class="col-lg-7">
                <div class="row">
                  <div class="col-lg-5" style="margin-right: -50px">
                    <h5>VRxD</h5>
                  </div>
                  <div class="col-lg-3">
                      <a class="github-button" href="https://github.com/dsaffo/VRxD" aria-label="Visit the repo"></a>
                  </div>
                </div>
                <p>VRxD is a collaborative cross-platform (desktop and VR) immersive analytics prototype for interactive data visualization in virtual reality. The system features advanced interaction sharing capabilities, enabling collaborative exploration of multi-dimensional datasets through coordinated views including strike zone visualizations, pitch trajectory analysis, and 3D spatial representations.</p>
              </div>
            </div>
          </div>


          <div class="project">
            <div class="row">
              <div class="col-lg-5 teaser">
                <a href="https://github.com/michaschwab/VisConnect" target="_blank">
                  <img src="https://camo.githubusercontent.com/9f4cd47d94407dc9495262bfdd94e7484a69c5e2788a2ed68031fe04e830796f/68747470733a2f2f692e696d6775722e636f6d2f4c666a6e7a654f2e676966" loading="lazy">
                </a>
              </div>
              <div class="col-lg-7">
                <div class="row">
                  <div class="col-lg-5" style="margin-right: -50px">
                    <a href="https://github.com/michaschwab/VisConnect" target="_blank">
                      <h5>VisConnect</h5>
                    </a>
                  </div>
                  <div class="col-lg-3">
                    <a class="github-button" href="https://github.com/michaschwab/VisConnect" aria-label="Visit the repo"></a>
                    <a class="github-button" href="https://github.com/michaschwab/VisConnect" data-icon="octicon-star" data-show-count="true" aria-label="Star Data Comets on GitHub"></a>
                  </div>
                </div>
                <p>Visconnect is a framework for adding p2p synchnronous collaberation to new or existing web-based visualizations. 
                  Events, such as clicking, dragging, and brushing, are synchronized across collaborators. 
                  Visconnect also supports synchronization of custom events to enable custom collaborative interactions.</p>
              </div>
            </div>
          </div>

          <div class="project">
            <div class="row">
              <div class="col-lg-5 teaser">
                <a href="https://www.datacomets.com/" target="_blank">
                  <img src="img/DataCometsWalkThrough.gif">
                </a>
              </div>
              <div class="col-lg-7">
                <div class="row">
                  <div class="col-lg-5" style="margin-right: -50px">
                    <a href="https://www.datacomets.com/" target="_blank">
                      <h5>Data Comets</h5>
                    </a>
                  </div>
                  <div class="col-lg-3">
                    <a class="github-button" href="https://github.com/dsaffo/DataComets" aria-label="Visit the repo"></a>
                    <a class="github-button" href="https://github.com/dsaffo/DataComets" data-icon="octicon-star" data-show-count="true" aria-label="Star Data Comets on GitHub"></a>
                  </div>
                </div>
                <p>Data Comets is an interactive PX4 flight log analysis tool. With Data Comets, you can encode flight data onto the flight path, filter and brush the data by time, and much more! This tool was developed to help UAV developers and operators analyze their autonomous aerial vehicle system data.</p>
              </div>
            </div>
          </div>
          <div class="project">
            <div class="row">
              <div class="col-lg-5 teaser">
             
                  <img src="img/https-netrdexplorerherokuappcom.gif" loading="lazy">
             
              </div>
              <div class="col-lg-7">
                <div class="row">
                  <div class="col-lg-5" style="margin-right: -50px">
                   
                      <h5>NETRD Explorer</h5>
                 
                  </div>
                  <div class="col-lg-3">
                    <a class="github-button" href="https://github.com/netsiphd/netrdexplorer" aria-label="Visit the repo"></a>
                    <a class="github-button" href="https://github.com/netsiphd/netrdexplorer" data-icon="octicon-star" data-show-count="true" aria-label="Star netrd explorer on GitHub"></a>
                  </div>
                </div>
                <p>NETRD Explorer is an interactive tool for exploring the functionality of the <a href="https://github.com/netsiphd/netrd">NETRD library</a>. NETRD is a python library for reconstructing networks from timeseries. With this tool you can explore several of the implemented methods and compare the results.</p>
              </div>
            </div>
          </div>
          <div class="project">
            <div class="row">
              <div class="col-lg-5 teaser">
                <a href="https://dsaffo.github.io/GeoSocialVis/" target="_blank">
                  <img src="img/GeoSocialVisVideoPreview_1046.gif" loading="lazy">
                </a>
              </div>
              <div class="col-lg-7">
                <div class="row">
                  <div class="col-lg-5" style="margin-right: -50px">
                    <a href="https://dsaffo.github.io/GeoSocialVis/" target="_blank">
                      <h5>GeoSocialVis</h5>
                    </a>
                  </div>
                  <div class="col-lg-3">
                    <a class="github-button" href="https://github.com/dsaffo/GeoSocialVis" aria-label="View the repo"></a>
                    <a class="github-button" href="https://github.com/dsaffo/GeoSocialVis" data-icon="octicon-star" data-show-count="true" aria-label="Star GeoSocialVis on GitHub"></a>
                  </div>
                </div>
                <p>GeoSocialVis, an interactive visualization tool for social network analyses with a focus on displaying the geosocial co-authorship network. GeoSocialVis uses a novel force layout that strikes a user-defined balance between showing network topology and the geographic locations of the nodes.</p>
              </div>
            </div>
          </div>
        </div>
        <div id="publications" class="publications">
          <h2>Publications</h2>
          <div class="line"></div>
          
          <div class="bibtex_structure">
            <div class="sort year" extra="DESC number">
              <div class="templates"></div>
            </div>
          </div>

          <textarea id="bibtex_input" style="display:none;">
          @article{datacomets,
  author    = {David Saffo and
               Aristotelis Leventidis and
               Twinkle Jain and
               Michelle A. Borkin and
               Cody Dunne},
  title     = {Data Comets: Designing a Visualization Tool for Analyzing Autonomous
               Aerial Vehicle Logs with Grounded Evaluation},
  journal   = {Comput. Graph. Forum},
  volume    = {39},
  number    = {3},
  pages     = {455--468},
  year      = {2020},
  url       = {https://doi.org/10.1111/cgf.13994},
  doi       = {10.1111/cgf.13994},
  timestamp = {Wed, 26 Aug 2020 11:04:47 +0200},
  biburl    = {https://dblp.org/rec/journals/cgf/SaffoLJBD20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  eprint = {https://osf.io/a4hfd/},
  Keywords = {article}
}

@inproceedings{DBLP:conf/chi/BartolomeoPLSSC20,
  author    = {Sara Di Bartolomeo and
               Aditeya Pandey and
               Aristotelis Leventidis and
               David Saffo and
               Uzma Haque Syeda and
               El{\'{\i}}n Carstensd{\'{o}}ttir and
               Magy Seif El{-}Nasr and
               Michelle A. Borkin and
               Cody Dunne},
  title     = {Evaluating the Effect of Timeline Shape on Visualization Task Performance},
  booktitle = {{CHI} '20: {CHI} Conference on Human Factors in Computing Systems,
               Honolulu, HI, USA, April 25-30, 2020},
  pages     = {1--12},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3313831.3376237},
  doi       = {10.1145/3313831.3376237},
  biburl    = {https://dblp.org/rec/conf/chi/BartolomeoPLSSC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  eprint = {https://osf.io/2kdb9/},
  Keywords = {article}
}

@article{DBLP:journals/titb/BorhaniKSNAZ19,
  author    = {Soheil Borhani and
               Justin Kilmarx and
               David Saffo and
               Lucien Ng and
               Reza Abiri and
               Xiaopeng Zhao},
  title     = {Optimizing Prediction Model for a Noninvasive Brain-Computer Interface
               Platform Using Channel Selection, Classification, and Regression},
  journal   = {{IEEE} J. Biomed. Health Informatics},
  volume    = {23},
  number    = {6},
  pages     = {2475--2482},
  year      = {2019},
  url       = {https://doi.org/10.1109/JBHI.2019.2892379},
  doi       = {10.1109/JBHI.2019.2892379},
  timestamp = {Sat, 30 May 2020 19:51:50 +0200},
  biburl    = {https://dblp.org/rec/journals/titb/BorhaniKSNAZ19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  Keywords = {article}
}

@article{schwab20-visconnect,
  author={Michail {Schwab} and David {Saffo} and Yixuan {Zhang} and Shash {Sinha} and Cristina {Nita-Rotaru} and James {Tompkin} and Cody {Dunne} and Michelle A. {Borkin}},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{VisConnect}: Distributed Event Synchronization for Collaborative Visualization}, 
  year={2020},
  doi={10.1109/TVCG.2020.3030366},
  eprint={https://osf.io/ut7e6},
  volume={},
  number={},
  pages={1-1},
  Keywords = {article}
}

@ARTICLE{9354592,
  author={Schwab, Michail and Saffo, David and Bond, Nicholas and Sinha, Shash and Dunne, Cody and Huang, Jeff and Tompkin, James and Borkin, Michelle},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Scalable Scalable Vector Graphics: Automatic Translation of Interactive SVGs to a Multithread VDOM for Fast Rendering}, 
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TVCG.2021.3059294},
  eprint={https://osf.io/ypxz2/},
  Keywords={article}
  }

@inproceedings{vrchat,
    author = {Saffo, David and Di Bartolomeo, Sara and Yildirim, Caglar and Dunne, Cody},
    title = {Remote and Collaborative Virtual Reality Experiments via Social VR Platforms},
    year = {2021},
    isbn = {9781450380966},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3411764.3445426},
    doi = {10.1145/3411764.3445426},
    booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
    articleno = {523},
    numpages = {15},
    keywords = {Replication Study, Quantitative Study, Qualitative study, Virtual Reality, Social VR, Transferability Study, Crowdsourcing},
    location = {Yokohama, Japan},
    series = {CHI '21},
    eprint={https://osf.io/3crhg},
    Keywords = {article}
    }





 @inproceedings{10.1145/3411764.3446866,
    author = {Ens, Barrett and Bach, Benjamin and Cordeil, Maxime and Engelke, Ulrich and Serrano, Marcos and Willett, Wesley and Prouzeau, Arnaud and Anthes, Christoph and B\"{u}schel, Wolfgang and Dunne, Cody and Dwyer, Tim and Grubert, Jens and Haga, Jason H. and Kirshenbaum, Nurit and Kobayashi, Dylan and Lin, Tica and Olaosebikan, Monsurat and Pointecker, Fabian and Saffo, David and Saquib, Nazmus and Schmalstieg, Dieter and Szafir, Danielle Albers and Whitlock, Matt and Yang, Yalong},
    title = {Grand Challenges in Immersive Analytics},
    year = {2021},
    isbn = {9781450380966},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3411764.3446866},
    doi = {10.1145/3411764.3446866},
    abstract = { Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.},
    booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
    articleno = {459},
    numpages = {17},
    keywords = {Immersive analytics, augmented reality, data visualisation, grand research challenges, virtual reality},
    location = {Yokohama, Japan},
    series = {CHI '21},
    eprint = {https://dl.acm.org/doi/pdf/10.1145/3411764.3446866},
    Keywords = {article}
    }


@inproceedings{10.1145/3411764.3445510,
    author = {South, Laura and Saffo, David and Borkin, Michelle A.},
    title = {Detecting and Defending Against Seizure-Inducing GIFs in Social Media},
    year = {2021},
    isbn = {9781450380966},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3411764.3445510},
    doi = {10.1145/3411764.3445510},
    abstract = {Despite recent improvements in online accessibility, the Internet remains an inhospitable place for users with photosensitive epilepsy, a chronic condition in which certain light stimuli can trigger seizures and even lead to death in extreme cases. In this paper, we explore how current risk detection systems have allowed attackers to take advantage of design oversights and target vulnerable users with photosensitivity on popular social media platforms. Through interviews with photosensitive individuals and a critical review of existing systems, we constructed design requirements for consumer-driven protective systems and developed a prototype browser extension for actively detecting and disarming potentially seizure-inducing GIFs and videos. We validate our system with a comprehensive dataset of simulated GIFs and GIFs collected from social media. Finally, we conduct a novel quantitative analysis of the prevalence of seizure-inducing GIFs across popular social media platforms and contribute recommendations for improving online accessibility for individuals with photosensitivity. All study materials are available at https://osf.io/5a3dy/.},
    booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
    articleno = {273},
    numpages = {17},
    keywords = {GIFs, photosensitive epilepsy, human-computer interaction, accessibility},
    location = {Yokohama, Japan},
    series = {CHI '21},
    eprint = {https://osf.io/4kgu6},
    Keywords = {article}
    }


%PrePrints

@article {Makarious2021.03.05.434104,
	author = {Makarious, Mary B. and Leonard, Hampton L. and Vitale, Dan and Iwaki, Hirotaka and Sargent, Lana and Dadu, Anant and Violich, Ivo and Hutchins, Elizabeth and Saffo, David and Bandres-Ciga, Sara and Kim, Jonggeol Jeff and Song, Yeajin and Bookman, Matt and Nojopranoto, Willy and Campbell, Roy H. and Hashemi, Sayed Hadi and Botia, Juan A. and Carter, John F. and Maleknia, Melina and Craig, David W. and Keuren-Jensen, Kendall Van and Morris, Huw R. and Hardy, John A. and Blauwendraat, Cornelis and Singleton, Andrew B. and Faghri, Faraz and Nalls, Mike A.},
	editor = {,},
	title = {Multi-Modality Machine Learning Predicting Parkinson{\textquoteright}s Disease},
	elocation-id = {2021.03.05.434104},
	year = {2021},
	doi = {10.1101/2021.03.05.434104},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Background Personalized medicine promises individualized disease prediction and treatment. The convergence of machine learning (ML) and available multi-modal data is key moving forward. We build upon previous work to deliver multi-modal predictions of Parkinson{\textquoteright}s Disease (PD).Methods We performed automated ML on multi-modal data from the Parkinson{\textquoteright}s Progression Marker Initiative (PPMI). After selecting the best performing algorithm, all PPMI data was used to tune the selected model. The model was validated in the Parkinson{\textquoteright}s Disease Biomarker Program (PDBP) dataset. Finally, networks were built to identify gene communities specific to PD.Findings Our initial model showed an area under the curve (AUC) of 89.72\% for the diagnosis of PD. The tuned model was then tested for validation on external data (PDBP, AUC 85.03\%). Optimizing thresholds for classification, increased the diagnosis prediction accuracy (balanced accuracy) and other metrics. Combining data modalities outperforms the single biomarker paradigm. UPSIT was the largest contributing predictor for the classification of PD. The transcriptomic data was used to construct a network of disease-relevant transcripts.Interpretation We have built a model using an automated ML pipeline to make improved multi-omic predictions of PD. The model developed improves disease risk prediction, a critical step for better assessment of PD risk. We constructed gene expression networks for the next generation of genomics-derived interventions. Our automated ML approach allows complex predictive models to be reproducible and accessible to the community.Funding National Institute on Aging, National Institute of Neurological Disorders and Stroke, the Michael J. Fox Foundation, and the Global Parkinson{\textquoteright}s Genetics Program.Evidence before this study Prior research into predictors of Parkinson{\textquoteright}s disease (PD) has either used basic statistical methods to make predictions across data modalities, or they have focused on a single data type or biomarker model. We have done this using an open-source automated machine learning (ML) framework on extensive multi-modal data, which we believe yields robust and reproducible results. We consider this the first true multi-modality ML study of PD risk classification.Added value of this study We used a variety of linear, non-linear, kernel, neural networks, and ensemble ML algorithms to generate an accurate classification of both cases and controls in independent datasets using data that is not involved in PD diagnosis itself at study recruitment. The model built in this paper significantly improves upon our previous models that used the entire training dataset in previous work1. Building on this earlier work, we showed that the PD diagnosis can be refined using improved algorithmic classification tools that may yield potential biological insights. We have taken careful consideration to develop and validate this model using public controlled-access datasets and an open-source ML framework to allow for reproducible and transparent results.Implications of all available evidence Training, validating, and tuning a diagnostic algorithm for PD will allow us to augment clinical diagnoses or risk assessments with less need for complex and expensive exams. Going forward, these models can be built on remote or asynchronously collected data which may be important in a growing telemedicine paradigm. More refined diagnostics will also increase clinical trial efficiency by potentially refining phenotyping and predicting onset, allowing providers to identify potential cases earlier. Early detection could lead to improved treatment response and higher efficacy. Finally, as part of our workflow, we built new networks representing communities of genes correlated in PD cases in a hypothesis-free manner, showing how new and existing genes may be connected and highlighting therapeutic opportunities.Competing Interest StatementHL, HI, FF, DV, YS, and MAN declare that they are consultants employed by Data Tecnica International, whose participation in this is part of a consulting agreement between the US National Institutes of Health and said company. HRM is employed by UCL. In the last 24 months he reports paid consultancy from Biogen, Biohaven, Lundbeck; lecture fees/honoraria from Wellcome Trust, Movement Disorders Society. Research Grants from Parkinsons UK, Cure Parkinsons Trust, PSP Association, CBD Solutions, Drake Foundation, Medical Research Council, Michael J Fox Foundation. HRM is also a co-applicant on a patent application related to C9ORF72 - Method for diagnosing a neurodegenerative disease (PCT/GB2012/052140).},
	URL = {https://www.biorxiv.org/content/early/2021/03/07/2021.03.05.434104},
	eprint = {https://www.biorxiv.org/content/early/2021/03/07/2021.03.05.434104.full.pdf},
	journal = {bioRxiv},
	Keywords = {article}
}


@misc{south_saffo_vitek_dunne_borkin_2022,
 title={Effective Use of Likert Scales in Visualization Evaluations: A Systematic Review},
 url={osf.io/6f3zs},
 DOI={10.31219/osf.io/6f3zs},
 publisher={OSF Preprints},
 author={South, Laura and Saffo, David and Vitek, Olga and Dunne, Cody and Borkin, Michelle},
 year={2022},
 month={Apr},
   location = {Rome, Italy},
    series = {EuroVis '22},
    eprint = {https://osf.io/4kgu6},
 Keywords = {article}
}

@article{10.1145/3770654,
author = {Gonzalez Penuela, Ricardo E. and Liu, Fannie and MacIntyre, Blair and Saffo, David},
title = {TapNav: Adaptive Spatiotactile Screen Readers for Tactually Guided Touchscreen Interactions for Blind and Low Vision People},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
url = {https://doi.org/10.1145/3770654},
doi = {10.1145/3770654},
abstract = {Screen readers are audio-based software that Blind and Low Vision (BLV) people use to interact with computing devices, such as tablets and smartphones. Although this technology has significantly improved the accessibility of touchscreen devices, the sequential nature of audio limits the bandwidth of information users can receive and process. We introduce TapNav, an adaptive spatiotactile screen reader prototype developed to interact with touchscreen interfaces spatially. TapNav's screen reader provides adaptive auditory feedback that, in combination with a tactile overlay, conveys spatial information and location of interface elements on-screen. We evaluated TapNav with 12 BLV users who interacted with TapNav to explore a data visualization and interact with a bank transactions application. Our qualitative findings show that touch points and spatially constrained navigation helped users anticipate outcomes for faster exploration, and offload cognitive load to touch. We provide design guidelines for creating tactile overlays for adaptive spatiotactile screen readers and discuss their generalizability beyond our exploratory data analysis and everyday application navigation scenarios.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {178},
numpages = {29},
keywords = {Accessibility, Blind People, Low Vision People, Multimodal Interaction, Spatial Interactions, Touchscreen, Gesture Input, Auditory Feedback, Tactile Cues, Screen reader, Touchscreen},
  Keywords = {article}
}

@article{10.1145/3711069,
author = {Liu, Fannie and Wang, Cheng Yao and Moriarty, William and Lu, Feiyu and Mir, Usman and Saffo, David and Chen, Mengyu and MacIntyre, Blair},
title = {SocialMiXR: Facilitating Hybrid Social Interactions at Conferences},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711069},
doi = {10.1145/3711069},
abstract = {Hybrid options at conferences, which support in-person and remote attendance, have increasingly become the norm in order to broaden participation and promote sustainability. However, hybrid conferences are challenging, where in-person and remote attendees often have disjoint, parallel experiences with limited opportunity to interact with each other. To explore the potential for facilitating social interaction between in-person and remote conference attendees, we designed and built SocialMiXR, a research prototype that uses WebXR technologies to align the physical and virtual worlds into one hybrid space for socialization. We deployed SocialMiXR in a three-day field study with 14 in-person and remote attendees of an engineering conference. Our qualitative results demonstrate that participants felt they were together in the same conference experience, and formed meaningful connections with each other. At the same time, they faced difficulties balancing different realities and capabilities given their separate contexts. We discuss implications for the design of hybrid social experiences at conferences.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW171},
numpages = {28},
keywords = {3d virtual environments, augmented reality, hybrid conferences, webxr},
  Keywords = {article}
}

@inproceedings{10.1145/3706598.3713346,
author = {Gottsacker, Matt and Chen, Mengyu and Saffo, David and Lu, Feiyu and Lee, Benjamin and MacIntyre, Blair},
title = {Examining the Effects of Immersive and Non-Immersive Presenter Modalities on Engagement and Social Interaction in Co-located Augmented Presentations},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713346},
doi = {10.1145/3706598.3713346},
abstract = {Head-worn augmented reality (AR) allows audiences to be immersed and engaged in stories told by live presenters. While presenters may also be in AR to have the same level of immersion and awareness as their audience, this symmetric presentation style may diminish important social cues such as eye contact. In this work, we examine the effects this (a)symmetry has on engagement, group awareness, and social interaction in co-located one-on-one augmented presentations. We developed a presentation system incorporating 2D/3D content that audiences can view and interact with in AR, with presenters controlling and delivering the presentation in either a symmetric style in AR, or an asymmetric style with a handheld tablet. We conducted a within- and between-subjects evaluation with 12 participant pairs to examine the differences between these symmetric and asymmetric presentation modalities. From our findings, we extracted four themes and derived strategies and guidelines for designers interested in augmented presentations.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1247},
numpages = {19},
keywords = {Augmented Reality, Augmented Presentation, Social Interaction, Engagement},
location = {
},
series = {CHI '25},
  Keywords = {article}
}

@ARTICLE{10568489,
author={Butcher, Peter W. S. and Batch, Andrea and Saffo, David and MacIntyre, Blair and Elmqvist, Niklas and Ritsos, Panagiotis D.},
journal={ IEEE Computer Graphics and Applications },
title={{ Is Native Naïve? Comparing Native Game Engines and WebXR as Immersive Analytics Development Platforms }},
year={2024},
volume={44},
number={03},
ISSN={1558-1756},
pages={91-98},
abstract={ Native game engines have long been the 3-D development platform of choice for research in mixed and augmented reality. For this reason, they have also been adopted in many immersive visualization and immersive analytics systems and toolkits. However, with the rapid improvements of WebXR and related open technologies, this choice may not always be optimal for future visualization research. In this article, we investigate common assumptions about native game engines versus WebXR and find that while native engines still have an advantage in many areas, WebXR is rapidly catching up and is superior for many immersive analytics applications. },
keywords={Visualization;Games;Augmented reality;Mixed reality;Immersive experience;Performance evaluation},
doi={10.1109/MCG.2024.3367422},
url = {https://doi.ieeecomputersociety.org/10.1109/MCG.2024.3367422},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=may,
  Keywords = {article}}

@article{https://doi.org/10.1111/cgf.15073,
author = {Di Bartolomeo, Sara and Crnovrsanin, Tarik and Saffo, David and Puerta, Eduardo and Wilson, Connor and Dunne, Cody},
title = {Evaluating Graph Layout Algorithms: A Systematic Review of Methods and Best Practices},
journal = {Computer Graphics Forum},
volume = {43},
number = {6},
pages = {e15073},
keywords = {visualization, graph drawing, graph layout algorithms, evaluation},
doi = {https://doi.org/10.1111/cgf.15073},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.15073},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15073},
abstract = {Abstract Evaluations—encompassing computational evaluations, benchmarks and user studies—are essential tools for validating the performance and applicability of graph and network layout algorithms (also known as graph drawing). These evaluations not only offer significant insights into an algorithm's performance and capabilities, but also assist the reader in determining if the algorithm is suitable for a specific purpose, such as handling graphs with a high volume of nodes or dense graphs. Unfortunately, there is no standard approach for evaluating layout algorithms. Prior work holds a ‘Wild West’ of diverse benchmark datasets and data characteristics, as well as varied evaluation metrics and ways to report results. It is often difficult to compare layout algorithms without first implementing them and then running your own evaluation. In this systematic review, we delve into the myriad of methodologies employed to conduct evaluations—the utilized techniques, reported outcomes and the pros and cons of choosing one approach over another. Our examination extends beyond computational evaluations, encompassing user-centric evaluations, thus presenting a comprehensive understanding of algorithm validation. This systematic review—and its accompanying website—guides readers through evaluation types, the types of results reported, and the available benchmark datasets and their data characteristics. Our objective is to provide a valuable resource for readers to understand and effectively apply various evaluation methods for graph layout algorithms. A free copy of this paper and all supplemental material is available at osf.io, and the categorized papers are accessible on our website at https://visdunneright.github.io/gd-comp-eval/.},
year = {2024},
  Keywords = {article}
}

@inproceedings{10.2312:eved.20241054,
booktitle = {EuroVis 2024 - Education Papers},
editor = {Firat, Elif E. and Laramee, Robert S. and Andersen, Nicklas Sindelv},
title = {{Vis Repligogy: Towards a Culture of Facilitating Replication Studies in Visualization Pedagogy and Research}},
author = {Syeda, Uzma Haque and South, Laura and Raynor, Justin and Panavas, Liudas and Saffo, David and Morriss, Tommy and Dunne, Cody and Borkin, Michelle A.},
year = {2024},
publisher = {The Eurographics Association},
ISBN = {978-3-03868-257-8},
DOI = {10.2312/eved.20241054},
  Keywords = {article}
}

@ARTICLE{IADesignSpace,
  author={Saffo, David and Di Bartolomeo, Sara and Crnovrsanin, Tarik and South, Laura and Raynor, Justin and Yildirim, Caglar and Dunne, Cody},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Unraveling the Design Space of Immersive Analytics: A Systematic Review}, 
  year={2024},
  volume={30},
  number={1},
  pages={495-506},
  keywords={Data visualization;Three-dimensional displays;Surveys;Systematics;Taxonomy;Market research;Collaboration;Immersive Analytics;Systematic Review;Survey;Augmented Reality;Virtual Reality;Design Space},
  doi={10.1109/TVCG.2023.3327368},
    Keywords = {article}
    }

@inproceedings{10.1145/3544548.3581093,
author = {Saffo, David and Batch, Andrea and Dunne, Cody and Elmqvist, Niklas},
title = {Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581093},
doi = {10.1145/3544548.3581093},
abstract = {Many collaborative data analysis situations benefit from collaborators utilizing different platforms. However, maintaining group awareness between team members using diverging devices is difficult, not least because common ground diminishes. A person using head-mounted VR cannot physically see a user on a desktop computer even while co-located, and the desktop user cannot easily relate to the VR user’s 3D workspace. To address this, we propose the “eyes-and-shoes” principles for group awareness and abstract them into four levels of techniques. Furthermore, we evaluate these principles with a qualitative user study of 6 participant pairs synchronously collaborating across distributed desktop and VR head-mounted devices. In this study, we vary the group awareness techniques between participants and explore two visualization contexts within participants. The results of this study indicate that the more visual metaphors and views of participants diverge, the greater the level of group awareness is needed. A copy of this paper, the study preregistration, and all supplemental materials required to reproduce the study are available on OSF (link).},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {383},
numpages = {15},
keywords = {Asymmetric collaboration, immersive analytics, ubiquitous analytics., virtual reality},
location = {Hamburg, Germany},
series = {CHI '23},
  Keywords = {article}
}

@ARTICLE{9904432,
  author={Raynor, Justin and Crnovrsanin, Tarik and Di Bartolomeo, Sara and South, Laura and Saffo, David and Dunne, Cody},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={The State of the Art in BGP Visualization Tools: A Mapping of Visualization Techniques to Cyberattack Types}, 
  year={2023},
  volume={29},
  number={1},
  pages={1059-1069},
  keywords={Visualization;Security;Routing;Systematics;Task analysis;Anomaly detection;Focusing;Visualization;Security;Routing;Systematics;Task analysis;Anomaly detection;Focusing},
  doi={10.1109/TVCG.2022.3209412},
    Keywords = {article}}

@phdthesis{Saffo2023MediumsMassesMethods,
  title        = {The Mediums, The Masses, The Methods: Towards Meeting the Demands of Immersive Analytics},
  author       = {David Saffo},
  year         = {2023},
  school       = {Northeastern University},
  doi          = {10.17760/D20560792},
  note         = {Ph.D.\ dissertation},
  url          = {https://doi.org/10.17760/D20560792},
    Keywords = {article}}



%Start Posters and Workshop

@misc{Shonan213_2024,
  title        = {Augmented Multimodal Interaction for Synchronous Presentation, Collaboration, and Education with Remote Audiences},
  year         = {2024},
  month        = jun,
  day          = {24--27},
  howpublished = {Shonan Village Center, NII Shonan Meeting \#213},
  note         = {Seminar on emerging multimodal interaction technologies for synchronous remote communication and collaboration around data},  
  url          = {https://shonan.nii.ac.jp/seminars/213/},
  Keywords = {poster}
}

@article{two,
 title={Two Dimensions for Organizing Immersive Analytics: Toward a Taxonomy for Facet and Position},
 url={osf.io/pk2rq},
 DOI={10.31219/osf.io/pk2rq},
 publisher={OSF Preprints},
 author={Saffo, David and Di Bartolomeo, Sara and Yildirim, Caglar and Dunne, Cody},
 year={2020},
 month={May},
 journal = {ACM CHI 2020 Workshop: Envisioning Future Productivity for Immersive Analytics (rooftop garden)},
 Keywords = {poster}
}

@inproceedings{DBLP:conf/chi/SaffoYBD20,
  author    = {David Saffo and
               Caglar Yildirim and
               Sara Di Bartolomeo and
               Cody Dunne},
  title     = {Crowdsourcing Virtual Reality Experiments using VRChat},
  booktitle = {Extended Abstracts of the 2020 {CHI} Conference on Human Factors in
               Computing Systems, {CHI} 2020, Honolulu, HI, USA, April 25-30, 2020},
  pages     = {1--8},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3334480.3382829},
  doi       = {10.1145/3334480.3382829},
  timestamp = {Tue, 12 May 2020 15:39:28 +0200},
  biburl    = {https://dblp.org/rec/conf/chi/SaffoYBD20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  keywords = {poster}
}

@article{geosocial,
 title={GeoSocialVis: Visualizing Geosocial Academic Co-Authorship Networks by Balancing Topology- and Geography- Based Layouts},
 url={osf.io/ykwah},
 DOI={10.31219/osf.io/ykwah},
 publisher={OSF Preprints},
 author={Saffo, David and Schwab, Michail and Borkin, Michelle and Dunne, Cody},
 year={2020},
 month={May},
 journal={IEEE Vis 2019 Poster},
 keywords={poster}
}

@inproceedings{saffo:hal-02005325,
  TITLE = {{Convolutional Neural Networks for a Cursor Control Brain Computer Interface}},
  AUTHOR = {Saffo, David and Kilmarx, Justin A and Borhani, Soheil and Abiri, Reza and Zhao, Xiaopeng and Albert, Mark V},
  URL = {https://hal.archives-ouvertes.fr/hal-02005325},
  BOOKTITLE = {{2018 Biomedical Engineering Society (BMES) Annual Meeting}},
  ADDRESS = {Atlanta, United States},
  YEAR = {2018},
  MONTH = Oct,
  HAL_ID = {hal-02005325},
  HAL_VERSION = {v1},
  Keywords = {poster}
}

@INPROCEEDINGS{10322252,
  author={Gottsacker, Matt and Chen, Mengyu and Saffo, David and Lu, Feiyu and MacIntyre, Blair},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Hybrid User Interface for Audience Feedback Guided Asymmetric Immersive Presentation of Financial Data}, 
  year={2023},
  volume={},
  number={},
  pages={199-204},
  keywords={Computers;Head-mounted displays;Federated learning;Multimedia systems;Sensor systems;Sensors;Augmented reality;H.5.1 [INFORMATION INTERFACES AND PRESENTATION (e.g., HCI)]: Multimedia Information Systems;Artificial;augmented;and virtual realities; K.4.3 [COMPUTERS AND SOCIETY]: Organizational Impacts;Computer-supported collaborative work},
  doi={10.1109/ISMAR-Adjunct60411.2023.00046},
    Keywords = {poster}
}

@INPROCEEDINGS{10536279,
  author={Wang, Cheng Yao and Saffo, David and Moriarty, Bill and Maclntyre, Blair},
  booktitle={2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={CollabXR: Bridging Realities in Collaborative Workspaces with Dynamic Plugin and Collaborative Tools Integration}, 
  year={2024},
  volume={},
  number={},
  pages={454-457},
  keywords={Training;Runtime;Codes;Extended reality;Collaboration;Mixed reality;Virtual environments;Human-centered computing-Collaborative interaction-;-Human-centered computing-Virtual reality-},
  doi={10.1109/VRW62533.2024.00089},
      Keywords = {poster}
}

@misc{ieeevr2024_t4_web_visualizations,
  title        = {Developing Immersive and Collaborative Visualizations with Web-Technologies},
  author       = {David Saffo and Cheng Yao Wang and Feiyu Lu and Blair MacIntyre},
  howpublished = {Tutorial at the 31st {IEEE} Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024)},
  year         = {2024},
  month        = mar,
  note         = {Orlando, Florida, USA},
  url          = {https://ieeevr.org/2024/program/tutorials/#T4},
      Keywords = {poster}
}

@misc{ieeevis2024_immersive_web_visualizations,
  title        = {Developing Immersive and Collaborative Visualizations with Web Technologies},
  author       = {David Saffo and Cheng Yao Wang and Feiyu Lu and Blair MacIntyre and Benjamin Lee},
  howpublished = {Tutorial at the IEEE Visualization Conference (IEEE VIS 2024)},
  year         = {2024},
  month        = oct,
  note         = {St. Pete Beach, Florida, USA},
  url          = {https://ieeevis.org/year/2024/info/tutorials#IMM},
      Keywords = {poster}
}

@misc{ieeevis2025_anujs_tutorial,
  title        = {Developing Immersive Visualizations and Interactions for the Web with Anu.js},
  author       = {David Saffo and Benjamin Lee and Cheng Yao Wang and Feiyu Lu and Blair MacIntyre},
  howpublished = {Tutorial at the IEEE Visualization Conference (IEEE VIS 2025)},
  year         = {2025},
  month        = nov,
  note         = {Vienna, Austria},
  url          = {https://ieeevis.org/year/2025/info/program/tutorials#ANU},
      Keywords = {poster}
}

@inproceedings{gottsacker2023_asymmetric_immersive_financial,
  title        = {Asymmetric Immersive Presentation System for Financial Data Visualization},
  author       = {Matt Gottsacker and Mengyu Chen and David Saffo and Feiyu Lu and Blair MacIntyre},
  booktitle    = {MERCADO: Multimodal Experiences for Remote Communication Around Data Online, Workshop at IEEE VIS 2023},
  year         = {2023},
  month        = oct,
  note         = {Workshop paper presented at IEEE VIS 2023, Melbourne, Australia},
  url          = {https://sites.google.com/view/mercadoworkshop/vis2023},
      Keywords = {poster}
}

%% Patents

@patent{US20250060869A1,
  title        = {Systems and methods for using stencils for multitouch interactive exploration},
  author       = {Saffo, David and Gonzalez, Ricardo and Liu, Fannie and MacIntyre, Blair},
  number       = {US20250060869A1},
  year         = {2025},
  month        = {02},
  day          = {20},
  type         = {Patent Application},
  assignee     = {JPMorgan Chase Bank, N.A.},
  url          = {https://patents.google.com/patent/US20250060869A1/en},
  Keywords = {patent}
}

@patent{US20250022020A1,
  title        = {Systems and methods for audience feedback guided mixed reality},
  author       = {Gottsacker, Matt and Chen, Mengyu and Saffo, David and Lu, Feiyu and MacIntyre, Blair},
  number       = {US20250022020A1},
  year         = {2025},
  month        = {01},
  day          = {16},
  type         = {Patent Application},
  assignee     = {JPMorgan Chase Bank N.A.},
  url          = {https://patents.google.com/patent/US20250022020A1/en},
    Keywords = {patent}
}






            
            
            </textarea>

            <div class="bibtex_display">
              <div class="bibtex_template">
                <div class="if author" style="margin-bottom: 20px;">
                  <h5><span class="title"></span>, <span class="year"></span></h5>
                  <p style="font-size: 14px; margin-bottom: 5px;">
                    <span class="author"><span class="first"></span> <span class="von"></span><span class="last"></span><span class="junior"></span></span>
                  </p>
                  <p style="font-size: 12px; margin-bottom: 5px;">
                    <span class="if journal"><em><span class="journal"></span></em>, </span>
                    <span class="if booktitle"><em><span class="booktitle"></span></em>, </span>
                    <span class="if volume">Vol. <span class="volume"></span>, </span>
                    <span class="if number">No. <span class="number"></span>, </span>
                    <span class="if pages">pp. <span class="pages"></span>, </span>
                    <span class="if publisher"><span class="publisher"></span>, </span>
                    <span class="year"></span>
                  </p>
                  <p style="font-size: 12px;">
                    <span class="if doi"><a class="doi" href="https://dx.doi.org/+DOI+" target="_blank" style="margin-right: 10px;">DOI</a></span>
                    <span class="if url"><a class="url" href="+URL+" target="_blank" style="margin-right: 10px;">URL</a></span>
                    <span class="if eprint"><a class="eprint" href="+EPRINT+" target="_blank" style="margin-right: 10px;">PDF</a></span>
                  </p>
                </div>
              </div>
            </div>

            <style>
              /* Bold David Saffo's name in publications */
              .bibtexentry .author span.David.Saffo .first,
              .bibtexentry .author span.David.Saffo .last {
                font-weight: bold;
              }
            </style>

        </div>
      </div>
    </div>
  </div>








  <!-- Boot Strap -->
 
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  <script async defer src="https://buttons.github.io/buttons.js"></script>
</body></html>
